{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2433753",
   "metadata": {},
   "source": [
    "# 1. [20 pts] In this work, we will improve our parsing and identifying keywords more than just a vocabulary. First step, let's improve our parsing through regular expressions: Extract words of length 2 or more, and digits of any length; but not words containing digits or digits containing words. Use any stop word list or lemmatizer as you see fit. Do not convert words to lower or upper case but keep them as they are for named entity recognition.(Suggestion: https://docs.python.org/3/howto/regex.html) Build a vocabulary and list the top 20 most frequent words in reviews grouped by sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbab35c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from nltk.corpus import reuters\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "\n",
    "# Combination of stop words and punctuations\n",
    "stop_words = stopwords.words('english') + list(punctuation)\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11dc31dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "def tokenize_review(review, _pat):\n",
    "    review = re.sub('<br />', ' ', review)\n",
    "    # Digits of any length, but not words containing digits or digits containing words\n",
    "    m = re.findall(_pat, review)\n",
    "    if len(m) > 0:\n",
    "        # Extract words of length 2 or more\n",
    "        m = [word for word in m if (len(word) > 2) and word.lower() not in stop_words]\n",
    "        return m\n",
    "    return []\n",
    "\n",
    "def findfun(tokenize_review,_pat, _lenm):\n",
    "    vocab_0 = []\n",
    "    vocab_1 = []\n",
    "    raw_0 = []\n",
    "    raw_1 = []\n",
    "    with open('../movie_data.csv', 'r', encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "        next(reader)  # Skip header\n",
    "\n",
    "        for line in reader:\n",
    "            review = line[0]\n",
    "            category = line[1]\n",
    "            tokens = tokenize_review(review, _pat)\n",
    "            if category == '0':\n",
    "                raw_0.append(review)\n",
    "                vocab_0.append(tokens)\n",
    "            elif category == '1':\n",
    "                raw_1.append(review)\n",
    "                vocab_1.append(tokens)\n",
    "                \n",
    "    return vocab_0,vocab_1,raw_0,raw_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c7fdcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 44.6 s\n",
      "Wall time: 59.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocab_0,vocab_1,raw_0,raw_1 = findfun(tokenize_review,r'[A-Za-z]+|[0-9]+',0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91565757",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_list_0 = [token for tokens in vocab_0 for token in tokens]\n",
    "flattened_list_1 = [token for tokens in vocab_1 for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e27dac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('movie', 49320),\n",
       " ('film', 37202),\n",
       " ('one', 24052),\n",
       " ('like', 21748),\n",
       " ('good', 14189),\n",
       " ('bad', 13895),\n",
       " ('would', 13865),\n",
       " ('even', 13401),\n",
       " ('time', 12051),\n",
       " ('really', 11862),\n",
       " ('see', 10439),\n",
       " ('story', 9877),\n",
       " ('get', 9866),\n",
       " ('much', 9839),\n",
       " ('make', 9230),\n",
       " ('could', 9137),\n",
       " ('people', 9018),\n",
       " ('made', 8720),\n",
       " ('movies', 8099),\n",
       " ('plot', 8011)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "cout_0=Counter(flattened_list_0)\n",
    "cout_0.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc480e37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('film', 41490),\n",
       " ('movie', 37289),\n",
       " ('one', 24701),\n",
       " ('like', 16901),\n",
       " ('good', 14342),\n",
       " ('story', 12500),\n",
       " ('time', 12496),\n",
       " ('great', 11954),\n",
       " ('see', 11801),\n",
       " ('well', 11500),\n",
       " ('really', 10472),\n",
       " ('would', 10461),\n",
       " ('also', 9416),\n",
       " ('much', 8907),\n",
       " ('first', 8530),\n",
       " ('even', 8366),\n",
       " ('people', 8277),\n",
       " ('get', 8104),\n",
       " ('love', 7984),\n",
       " ('best', 7738)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(flattened_list_1).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3793fbd2",
   "metadata": {},
   "source": [
    "# 2. [20 pts] Generate the 20 top frequent bigrams in reviews grouped by sentiment. (Hint: Update only the tokenizer. Have you used CountVectorizer?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5a9d564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk import bigrams\n",
    "all_bigrams=[]\n",
    "for review in raw_0:\n",
    "    tokenized_review = tokenize_review(review,r'[A-Za-z]+|[0-9]+')\n",
    "    all_bigrams.extend(nltk.bigrams(tokenized_review))\n",
    "\n",
    "    \n",
    "bigram_counter = Counter(all_bigrams)\n",
    "most_common0=bigram_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e66c7546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk import bigrams\n",
    "all_bigrams=[]\n",
    "for review in raw_1:\n",
    "    tokenized_review = tokenize_review(review,r'[A-Za-z]+|[0-9]+')\n",
    "    all_bigrams.extend(nltk.bigrams(tokenized_review))\n",
    "\n",
    "    \n",
    "bigram_counter = Counter(all_bigrams)\n",
    "most_common1=bigram_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b3f94c",
   "metadata": {},
   "source": [
    "# 3. [20 pts] Generate the 20 top frequent bigrams, which are 'NN' POS tagged in reviews grouped by sentiment.\n",
    "# (Hint: Generate strictly bigrams, i.e., two words tagged 'NN' next to each other)\n",
    "# Compare and contrast the two rankings generated in questions (2.) and (3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2fa499a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\mjhig\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57066e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text once\n",
    "tokenized_reviews0 = [tokenize_review(review, r'[A-Za-z]+|[0-9]+') for review in raw_0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1baeaf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through tokenized reviews and extract bigrams\n",
    "tags_NN_0 = []\n",
    "pos_tags=[nltk.pos_tag(token) for token in tokenized_reviews0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1ab6399",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_full_list0=[]\n",
    "for token_set in pos_tags:\n",
    "    for x in range(len(token_set)-1):\n",
    "        tag_1=token_set[x][0]\n",
    "        tag_2=token_set[x+1][0]\n",
    "        if(token_set[x][1] == 'NN' and token_set[x+1][1] == 'NN' and tag_1.lower() != 'none' and tag_2.lower() != 'none'):\n",
    "            NN_full_list0.append(token_set[x][0]+','+token_set[x+1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c719b8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common0NN = Counter(NN_full_list0).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35551dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_reviews1 = [tokenize_review(review, r'[A-Za-z]+|[0-9]+') for review in raw_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "150b6c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags2=[nltk.pos_tag(token) for token in tokenized_reviews1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1da23517",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NN_full_list1=[]\n",
    "for token_set in pos_tags:\n",
    "    for x in range(len(token_set)-1):\n",
    "        tag_1=token_set[x][0]\n",
    "        tag_2=token_set[x+1][0]\n",
    "        if(token_set[x][1] == 'NN' and token_set[x+1][1] == 'NN' and tag_1.lower() != 'none' and tag_2.lower() != 'none'):\n",
    "            NN_full_list1.append(token_set[x][0]+','+token_set[x+1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb3f388a",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common1NN = Counter(NN_full_list1).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "837612e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarities {'waste,time', 'watch,movie', 'horror,movie'}\n",
      "Only in regular bigrams 17\n",
      "Only in NN bigrams 17\n",
      "similarities set()\n",
      "Only in regular bigrams 20\n",
      "Only in NN bigrams 20\n"
     ]
    }
   ],
   "source": [
    "def find_differences(most_common,most_commonNN):\n",
    "    phrases_list1 = [item[0][0]+','+item[0][1] for item in most_common]\n",
    "    phrases_list2 = [item[0] for item in most_commonNN]\n",
    "    phrases_list1.sort()\n",
    "    phrases_list2.sort()\n",
    "    phrases_set1=set(phrases_list1)\n",
    "    phrases_set2=set(phrases_list2)\n",
    "    print('similarities',phrases_set1.intersection(phrases_set2))\n",
    "    print('Only in regular bigrams',len(phrases_set1.difference(phrases_set2)))\n",
    "    print('Only in NN bigrams',len(phrases_set2.difference(phrases_set1)))\n",
    "\n",
    "        \n",
    "find_differences(most_common0,most_common0NN)\n",
    "find_differences(most_common1,most_common1NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d31ee0b",
   "metadata": {},
   "source": [
    "##  After Analyzing the results, it is apparent that there is a large difference between the most common bigrams that are not NN tagged and the ones that are NN tagged. for Sentiment zero a mere 3 out of the top 20 were shared between the NN-list and the regular list. Between sentiment 1, there were NO similarities between th"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1cace5",
   "metadata": {},
   "source": [
    "# 4. [20 pts] Generate all the 4-grams that have counts 2 or more in reviews grouped by sentiment. (Hint: To keep track of N-grams use a Python dictionary which has the key as, Key = word0+' '+word1+' '+word2+' '+word3) What is the top frequent 4-gram in sentiment 0 and 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53bb4e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_4grams(tokens):\n",
    "    return [tokens[i:i+4] for i in range(len(tokens) - 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "113a19f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the top frequent 4gram in sentiment 0: [(('worst', 'movie', 'ever', 'seen'), 235)]\n",
      "the top frequent 4gram in sentiment 1: [(('one', 'best', 'movies', 'ever'), 46)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def get_all_4grams(tokenized_reviews):\n",
    "    all_four_grams=[]\n",
    "    for review in tokenized_reviews:\n",
    "        four_grams=generate_4grams(review)\n",
    "        for gram in four_grams:\n",
    "            all_four_grams.append(tuple(gram))\n",
    "    return Counter(all_four_grams)\n",
    "four_grams0=get_all_4grams(tokenized_reviews0)\n",
    "filtered_4grams0 = {k: v for k,v in four_grams0.items() if int(v) > 1}\n",
    "four_grams1=get_all_4grams(tokenized_reviews1)\n",
    "filtered_4grams1 = {k: v for k,v in four_grams1.items() if int(v) > 1}\n",
    "print('the top frequent 4gram in sentiment 0:',four_grams0.most_common(1))\n",
    "print('the top frequent 4gram in sentiment 1:',four_grams1.most_common(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7017d164",
   "metadata": {},
   "source": [
    "# 5. [20 pts] In this dataset identify,\n",
    "# • the most probable word that would come after \"worst film ever\"?\n",
    "# • the most probable word that would come after \"best movie ever\"?\n",
    "# What are the probabilities of words that come after,\n",
    "# • \"worst film ever\"?\n",
    "# • \"best movie ever\"?\n",
    "# Comment about any value of N-grams approach in this movie reviews context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7505e91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase1:\n",
      "the most common word after ('worst', 'film', 'ever') is: seen\n",
      "seen 59.51%\n",
      "made 27.61%\n",
      "see 1.23%\n",
      "watched 1.23%\n",
      "viewed 1.23%\n",
      "created 1.84%\n",
      "acting 1.84%\n",
      "saw 1.84%\n",
      "witnessed 1.23%\n",
      "misfortune 1.23%\n",
      "paid 1.23%\n",
      "\n",
      "\n",
      "Phrase2:\n",
      "the most common word after ('best', 'movie', 'ever') is: seen\n",
      "seen 70.00%\n",
      "made 30.00%\n"
     ]
    }
   ],
   "source": [
    "def phrase_check(phrase2):\n",
    "    total_count=0\n",
    "    best_movie_ever={}\n",
    "    for item in filtered_4grams0.items():\n",
    "        if(item[0][:3] == phrase2):\n",
    "            total_count+=item[1]\n",
    "            best_movie_ever[str(item[0][3])]=item[1]\n",
    "    for item in filtered_4grams1.items():\n",
    "        if(item[0][:3] == phrase2):\n",
    "            total_count+=item[1]\n",
    "            if(str(item[0][3]) in best_movie_ever.keys()):\n",
    "                best_movie_ever[str(item[0][3])]+=item[1]\n",
    "            else:\n",
    "                best_movie_ever[str(item[0][3])]=item[1]\n",
    "    print('the most common word after',phrase2,'is:',max(best_movie_ever, key=lambda k: best_movie_ever[k]))\n",
    "    for item in best_movie_ever.items():\n",
    "        print(item[0],f'{(item[1]/total_count) * 100:.2f}%')\n",
    "print('Phrase1:')\n",
    "phrase_check(('worst','film','ever'))\n",
    "print(\"\\n\")\n",
    "print('Phrase2:')\n",
    "phrase_check(('best','movie','ever'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6654f872",
   "metadata": {},
   "source": [
    "# The N-grams Approach can provide us valuable insights into the types vocabulary and words that are used to follow another preceding set of words. For example, we can see that \"seen\" is nearly  60% of all of the entries of phrase1, as well as 70% of the words in phrase2. In terms of n-gram tagging, since, as we have seen in question number 4, that the n-gram \"worst film ever seen\" is also the most common 4gram out of all of the 4grams. in this respect, we may be able to actually better predict the outcome of phrase1 since the perplexity is lower. This is even though the word seen has a lower probability anyway."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
